{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0dlMmgOoSys0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.base import clone\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "import copy\n",
        "from importlib import import_module\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
        "from collections import OrderedDict\n",
        "import copy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ji3M3m4gm1wW"
      },
      "outputs": [],
      "source": [
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DieiaAzXm1wX"
      },
      "outputs": [],
      "source": [
        "class RBM(nn.Module):\n",
        "\n",
        "    def __init__(self, n_visible, n_hidden, lr=1e-3, epochs=10, batch_size=30, k=3, use_gpu=True, verbose=True):\n",
        "        super(RBM, self).__init__()\n",
        "\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hidden = n_hidden\n",
        "        self.lr = torch.tensor(lr, dtype=torch.float32)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.k = k\n",
        "        self.use_gpu = use_gpu\n",
        "        self.verbose = verbose\n",
        "\n",
        "        # Set the device to GPU if available\n",
        "        if torch.cuda.is_available() and use_gpu==True:\n",
        "            dev = \"cuda:0\"\n",
        "        else:\n",
        "            dev = \"cpu\"\n",
        "        self.device_ = torch.device(dev)\n",
        "\n",
        "        # Initialise weights and biases\n",
        "        std = 4 * np.sqrt(6. / (self.n_visible + self.n_hidden))\n",
        "        self.W = torch.normal(mean=0, std=std, size=(self.n_visible, self.n_hidden))\n",
        "        self.vb = torch.zeros(self.n_visible)\n",
        "        self.hb = torch.zeros(self.n_hidden)\n",
        "\n",
        "        self.W = self.W.to(self.device_)\n",
        "        self.vb = self.vb.to(self.device_)\n",
        "        self.hb = self.hb.to(self.device_)\n",
        "\n",
        "    def v_to_h(self, v):\n",
        "        h = torch.matmul(v,self.W)   # calculate the activations of hidden units\n",
        "        h = torch.add(h, self.hb)    # add bias term to the activations\n",
        "        h = torch.sigmoid(h)\n",
        "        return h, torch.bernoulli(h) # return both the probabilities and binary samples of hidden layer units\n",
        "\n",
        "    def h_to_v(self, h):\n",
        "        v = torch.matmul(h,self.W.t())  # calculate the activations of visible units\n",
        "        v = torch.add(v, self.vb)       # add bias term to the activations\n",
        "        v = torch.sigmoid(v)\n",
        "        return v, torch.bernoulli(v)    # return both the probabilities and binary samples of visible layer units\n",
        "\n",
        "    def contrastive_divergence(self, v0):\n",
        "        # initial activations of hidden units and hidden samples using the input data\n",
        "        h0, hkact = self.v_to_h(v0)\n",
        "\n",
        "        # perform gibbs sampling k times to get the final samples of hidden and visible units\n",
        "        for i in range(self.k):\n",
        "            vk, _ = self.h_to_v(hkact)\n",
        "            hk, hkact = self.v_to_h(vk)\n",
        "\n",
        "        # compute delta for the parameters using the input and the final samples of hidden and visible units\n",
        "        dW = torch.mm(v0.t(), h0) - torch.mm(vk.t(), hk) # delta for W\n",
        "        dvb = torch.sum((v0-vk), 0) # delta for visible unit biases\n",
        "        dhb = torch.sum((h0-hk), 0) # delta for hidden unit biases\n",
        "\n",
        "        # update the parameters using the computed deltas\n",
        "        self.W += self.lr * dW\n",
        "        self.vb += self.lr * dvb\n",
        "        self.hb += self.lr * dhb\n",
        "\n",
        "        # compute the reconstruction error between the input and the final reconstructed visible layer\n",
        "        err = torch.mean(torch.sum((v0 - vk)**2, 0))\n",
        "        return err\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.v_to_h(X)\n",
        "\n",
        "    def train(self, dataset):\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size)\n",
        "\n",
        "        for ep in range(self.epochs):\n",
        "            running_cost = 0.\n",
        "            n_batchs = 0\n",
        "            for i, (batch, _) in enumerate(loader):\n",
        "                batch = batch.view(len(batch), self.n_visible)\n",
        "                running_cost += self.contrastive_divergence(batch)\n",
        "                n_batchs += 1\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f'RBM - Epoch: {ep}, averaged cost = {running_cost/n_batchs}')\n",
        "        return\n",
        "\n",
        "class GBRBM(RBM):\n",
        "    def h_to_v(self,h):\n",
        "\n",
        "        v = torch.matmul(h ,self.W.t()) # calculate the activations of visible units\n",
        "        v = torch.add(v, self.vb)       # add bias term to the activations\n",
        "\n",
        "        # return both the probabilities and gaussian samples of visible layer units\n",
        "        return v, v + torch.normal(mean=0, std=1, size=v.shape).to(self.device_)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XMdoyvFm1wX"
      },
      "outputs": [],
      "source": [
        "class DBN(nn.Module):\n",
        "    def __init__(self, n_visible, n_hiddens, lr=1e-3, epochs=100, batch_size=50, k=3,\n",
        "                 use_gpu=True, verbose=True):\n",
        "        super(DBN,self).__init__()\n",
        "\n",
        "        self.n_layers = len(n_hiddens)\n",
        "        self.n_visible = n_visible\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.use_gpu = use_gpu\n",
        "        self.verbose = verbose\n",
        "        self.lr = torch.tensor(lr, dtype=torch.float32)\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = int(batch_size)\n",
        "        self.k = k\n",
        "\n",
        "        self.rbm_layers_ = []\n",
        "        for i in range(self.n_layers):\n",
        "            if i == 0:\n",
        "                n_in = n_visible\n",
        "                rbm = GBRBM(\n",
        "                    n_in,\n",
        "                    n_hiddens[0],\n",
        "                    lr=lr,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    k=k,\n",
        "                    use_gpu=use_gpu,\n",
        "                    verbose=verbose\n",
        "                )\n",
        "            else:\n",
        "                n_in = n_hiddens[i-1]\n",
        "                rbm = RBM(n_in,\n",
        "                    n_hiddens[i],\n",
        "                    lr=lr,\n",
        "                    epochs=epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    k=k,\n",
        "                    use_gpu=use_gpu,\n",
        "                    verbose=verbose\n",
        "                )\n",
        "            self.rbm_layers_.append(rbm)\n",
        "\n",
        "    def forward(self, X):\n",
        "        h = torch.as_tensor(X, dtype=torch.float)\n",
        "        for rbm in self.rbm_layers_:\n",
        "            h = h.view((h.shape[0], -1)) # flatten\n",
        "            p_h, h = rbm.v_to_h(h)\n",
        "        return p_h, h\n",
        "\n",
        "    def pre_train(self, X):\n",
        "        y = torch.zeros(X.shape[0])\n",
        "\n",
        "        # train RBMs layer by layer\n",
        "        for rbm in self.rbm_layers_:\n",
        "            dataset = TensorDataset(X, y)\n",
        "            rbm.train(dataset)\n",
        "\n",
        "            # forward to next rbm\n",
        "            X = X.view((X.shape[0], -1)) # flatten\n",
        "            _, X = rbm.forward(X)\n",
        "\n",
        "    def to_autoencoder(self, loss='MSELoss', optimizer='RMSprop', lr=0.01,\n",
        "                       epochs=50, batch_size=50, loss_kwargs={}, optimizer_kwargs=dict()):\n",
        "        return AEDBN(self, loss=loss, optimizer=optimizer, lr=lr, epochs=epochs, batch_size=batch_size,\n",
        "                     loss_kwargs=loss_kwargs, optimizer_kwargs=optimizer_kwargs, verbose=self.verbose)\n",
        "\n",
        "class AEDBN(nn.Module):\n",
        "\n",
        "    def __init__(self, dbn, loss='MSELoss', optimizer='RMSprop', lr=0.01, epochs=50, batch_size=50,\n",
        "                 loss_kwargs={}, optimizer_kwargs=dict(), verbose=True):\n",
        "        super(AEDBN,self).__init__()\n",
        "\n",
        "        self.dbn = dbn\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.loss_kwargs = loss_kwargs\n",
        "        self.optimizer_kwargs = optimizer_kwargs\n",
        "\n",
        "        self.construct_autoencoder()\n",
        "\n",
        "\n",
        "    def construct_autoencoder(self):\n",
        "        # unroll as an anto encoder/decoder\n",
        "        n_in = self.dbn.n_visible\n",
        "\n",
        "        # encoder part\n",
        "        modules = []\n",
        "        for n_hidden, rbm in zip(self.dbn.n_hiddens, self.dbn.rbm_layers_):\n",
        "            layer = nn.Linear(n_in, n_hidden)\n",
        "            layer.weight, layer.bias = nn.Parameter(rbm.W.t()), nn.Parameter(rbm.hb)\n",
        "            modules.append(layer)\n",
        "            modules.append(nn.Sigmoid())\n",
        "            n_in = n_hidden\n",
        "        self.encoder_ = nn.Sequential(*modules)\n",
        "\n",
        "        # decoder part\n",
        "        modules = []\n",
        "        for i, n_hidden in enumerate(reversed(self.dbn.n_hiddens)):\n",
        "            if i > 0:\n",
        "                layer = nn.Linear(n_in, n_hidden)\n",
        "                layer.weight, layer.bias = nn.Parameter(self.dbn.rbm_layers_[-i].W), nn.Parameter(self.dbn.rbm_layers_[-i].vb)\n",
        "                modules.append(layer)\n",
        "                modules.append(nn.Sigmoid())\n",
        "                n_in = n_hidden\n",
        "        layer = nn.Linear(n_hidden, self.dbn.n_visible)\n",
        "        layer.weight, layer.bias = nn.Parameter(self.dbn.rbm_layers_[0].W), nn.Parameter(self.dbn.rbm_layers_[0].vb)\n",
        "        modules.append(layer) # final output layer\n",
        "        self.decoder_ = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = torch.as_tensor(X, dtype=torch.float)\n",
        "        enc = self.encoder_(X)\n",
        "        return self.decoder_(enc)\n",
        "\n",
        "    def fine_tune(self, X, y=None):\n",
        "        X = torch.as_tensor(X, dtype=torch.float)\n",
        "        if y is None:\n",
        "            y = X.detach().clone()\n",
        "\n",
        "        # create dataset and data loader\n",
        "        dataset = TensorDataset(X, y)\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size)\n",
        "\n",
        "        # set loss function and optimizer\n",
        "        loss_fn = getattr(import_module('torch.nn'), self.loss)(**self.loss_kwargs)\n",
        "        optimizer = getattr(import_module('torch.optim'), self.optimizer)\n",
        "        optimizer = optimizer(self.parameters(), lr=self.lr, **self.optimizer_kwargs)\n",
        "\n",
        "        # switch to training mode\n",
        "        self.train(True)\n",
        "\n",
        "        # train the model\n",
        "        for ep in range(self.epochs):\n",
        "            running_loss = 0.\n",
        "\n",
        "            for i, (batch, y_batch) in enumerate(loader):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.forward(batch)\n",
        "                loss = loss_fn(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            avg_loss = running_loss / len(loader)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f'AEDBN - Epoch {ep}, loss_train={avg_loss}')\n",
        "\n",
        "        # switch back to evaluation mode\n",
        "        self.train(False)\n",
        "\n",
        "    def to_clf(self, n_class=5, loss='CrossEntropyLoss', optimizer='RMSprop', lr=0.01,\n",
        "               epochs=50, batch_size=50, loss_kwargs={}, optimizer_kwargs=dict()):\n",
        "        return CDBN(copy.deepcopy(self.encoder_), self.dbn.n_hiddens[-1], n_class=n_class, loss=loss,\n",
        "                    optimizer=optimizer, lr=lr, epochs=epochs, batch_size=batch_size,\n",
        "                    loss_kwargs=loss_kwargs, optimizer_kwargs=optimizer_kwargs, verbose=self.verbose)\n",
        "\n",
        "class CDBN(nn.Module):\n",
        "\n",
        "    def __init__(self, encoder, encode_size, n_class=5, loss='CrossEntropyLoss', optimizer='RMSprop', lr=0.01,\n",
        "                 epochs=50, batch_size=50, loss_kwargs={}, optimizer_kwargs=dict(), verbose=True):\n",
        "        super(CDBN,self).__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.encode_size = encode_size\n",
        "        self.n_class = n_class\n",
        "        self.loss = loss\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.verbose = verbose\n",
        "        self.loss_kwargs = loss_kwargs\n",
        "        self.optimizer_kwargs = optimizer_kwargs\n",
        "\n",
        "        self.output_layer_ = nn.Linear(self.encode_size, n_class)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = torch.as_tensor(X, dtype=torch.float)\n",
        "        return self.output_layer_(self.encoder(X))\n",
        "\n",
        "    def fine_tune(self, X, y):\n",
        "        X = torch.as_tensor(X, dtype=torch.float)\n",
        "\n",
        "        # Create a DataLoader object to efficiently load the data in batches\n",
        "        dataset = TensorDataset(X, y)\n",
        "        loader = DataLoader(dataset, batch_size=self.batch_size)\n",
        "\n",
        "        # Import the loss function and optimizer\n",
        "        loss_fn = getattr(import_module('torch.nn'), self.loss)(**self.loss_kwargs)\n",
        "        optimizer = getattr(import_module('torch.optim'), self.optimizer)\n",
        "        optimizer = optimizer(self.parameters(), lr=self.lr, **self.optimizer_kwargs)\n",
        "\n",
        "        # Set the model to training mode\n",
        "        self.train(True)\n",
        "\n",
        "        # Iterate over the epochs\n",
        "        for ep in range(self.epochs):\n",
        "            running_loss = 0.\n",
        "\n",
        "            for i, (batch, y_batch) in enumerate(loader):\n",
        "                optimizer.zero_grad()\n",
        "                outputs = self.forward(batch)\n",
        "                loss = loss_fn(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_loss += loss.item()\n",
        "            avg_loss = running_loss / len(loader)\n",
        "\n",
        "            if self.verbose:\n",
        "                print(f'CDBN - Epoch {ep}, loss_train={avg_loss}')\n",
        "\n",
        "        # Set the model to evaluation mode\n",
        "        self.train(False)\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        X = torch.as_tensor(X, dtype=torch.float)\n",
        "\n",
        "        self.eval()\n",
        "        outputs = self.forward(X)\n",
        "        softmax = nn.LogSoftmax(dim=1)\n",
        "        _, y_pred = torch.max(softmax(outputs), dim=1)\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ElyEuE_zm1wZ"
      },
      "outputs": [],
      "source": [
        "class DBNClassifier(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, n_hiddens=[500, 100, 20], k=3, loss_ae='MSELoss', loss_clf='CrossEntropyLoss',\n",
        "                 optimizer_ae='RMSprop', optimizer_clf='RMSprop',\n",
        "                 lr_rbm=1e-3, lr_ae=0.01, lr_clf=0.01,\n",
        "                 epochs_rbm=100, epochs_ae=50, epochs_clf=50,\n",
        "                 batch_size_rbm=50, batch_size_ae=30, batch_size_clf=50,\n",
        "                 loss_ae_kwargs={}, loss_clf_kwargs={},\n",
        "                 optimizer_ae_kwargs={}, optimizer_clf_kwargs={}, random_state=42,\n",
        "                 use_gpu=True, verbose=True):\n",
        "\n",
        "        self.n_layers = len(n_hiddens)\n",
        "        self.n_hiddens = n_hiddens\n",
        "        self.k = k\n",
        "        self.loss_ae = loss_ae\n",
        "        self.loss_clf = loss_clf\n",
        "        self.optimizer_ae = optimizer_ae\n",
        "        self.optimizer_clf = optimizer_clf\n",
        "        self.lr_rbm = lr_rbm\n",
        "        self.lr_ae = lr_ae\n",
        "        self.lr_clf = lr_clf\n",
        "        self.epochs_rbm = epochs_rbm\n",
        "        self.epochs_ae = epochs_ae\n",
        "        self.epochs_clf = epochs_clf\n",
        "        self.batch_size_rbm = int(batch_size_rbm)\n",
        "        self.batch_size_ae = int(batch_size_ae)\n",
        "        self.batch_size_clf = int(batch_size_clf)\n",
        "        self.use_gpu = use_gpu\n",
        "        self.verbose = verbose\n",
        "        self.loss_ae_kwargs = loss_ae_kwargs\n",
        "        self.loss_clf_kwargs = loss_clf_kwargs\n",
        "        self.optimizer_ae_kwargs = optimizer_ae_kwargs\n",
        "        self.optimizer_clf_kwargs = optimizer_clf_kwargs\n",
        "        self.random_state = random_state\n",
        "        self.le_ = LabelEncoder()\n",
        "\n",
        "\n",
        "\n",
        "        if torch.cuda.is_available() and use_gpu==True:\n",
        "            dev = \"cuda:0\"\n",
        "        else:\n",
        "            dev = \"cpu\"\n",
        "        self.device_ = torch.device(dev)\n",
        "\n",
        "        if not self.random_state is None:\n",
        "            torch.manual_seed(self.random_state)\n",
        "\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        try:\n",
        "          X, y = check_X_y(X, y)\n",
        "          self.n_features_in_ = X.shape[1]\n",
        "          self.n_classes_ = np.unique(y).shape[0]\n",
        "\n",
        "          self.le_.fit(y)\n",
        "\n",
        "          X = torch.as_tensor(X, dtype=torch.float).to(self.device_)\n",
        "          y = torch.as_tensor(self.le_.transform(y), dtype=torch.int64).to(self.device_)\n",
        "\n",
        "          # pretrain DBN\n",
        "          dbn = DBN(\n",
        "              self.n_features_in_,\n",
        "              self.n_hiddens,\n",
        "              lr=self.lr_rbm,\n",
        "              epochs=self.epochs_rbm,\n",
        "              batch_size=self.batch_size_rbm,\n",
        "              k=self.k,\n",
        "              use_gpu=self.use_gpu,\n",
        "              verbose=self.verbose\n",
        "          )\n",
        "          dbn.pre_train(X)\n",
        "\n",
        "          # unroll as autoencoder-decoder and fine tune\n",
        "          self.aedbn_ = dbn.to_autoencoder(\n",
        "              loss=self.loss_ae,\n",
        "              optimizer=self.optimizer_ae,\n",
        "              lr=self.lr_ae,\n",
        "              epochs=self.epochs_ae,\n",
        "              batch_size=self.batch_size_ae,\n",
        "              loss_kwargs=self.loss_ae_kwargs,\n",
        "              optimizer_kwargs=self.optimizer_ae_kwargs\n",
        "          )\n",
        "          self.aedbn_.to(self.device_)\n",
        "          self.aedbn_.fine_tune(X)\n",
        "\n",
        "          # use the trained encoder, add an output layer and perform a supervised fine-tune\n",
        "          self.cdbn_ = self.aedbn_.to_clf(\n",
        "              n_class=self.n_classes_,\n",
        "              loss=self.loss_clf,\n",
        "              optimizer=self.optimizer_clf,\n",
        "              lr=self.lr_clf,\n",
        "              epochs=self.epochs_clf,\n",
        "              batch_size=self.batch_size_clf,\n",
        "              loss_kwargs=self.loss_clf_kwargs,\n",
        "              optimizer_kwargs=self.optimizer_clf_kwargs\n",
        "          )\n",
        "          self.cdbn_.to(self.device_)\n",
        "          self.cdbn_.fine_tune(X, y)\n",
        "\n",
        "          return self\n",
        "        except Exception as e:\n",
        "          print(f\"An error occurred during fitting: {e}\")\n",
        "          raise\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        check_is_fitted(self)\n",
        "\n",
        "        # Input validation\n",
        "        X = check_array(X)\n",
        "        X = torch.as_tensor(X).to(self.device_)\n",
        "\n",
        "        return self.le_.inverse_transform(self.cdbn_.predict(X).cpu().detach().numpy())\n",
        "\n",
        "class SimpleDBNClassifier(DBNClassifier):\n",
        "    def __init__(self, n_hiddens=..., lr_pre_train=1e-5, lr_fine_tune=0.01,\n",
        "                 epochs_pre_train=10, epochs_fine_tune=5, batch_size=30, k=3,\n",
        "                 random_state=42, use_gpu=True, verbose=True):\n",
        "        self.lr_pre_train = lr_pre_train\n",
        "        self.lr_fine_tune = lr_fine_tune\n",
        "        self.epochs_pre_train = epochs_pre_train\n",
        "        self.epochs_fine_tune = epochs_fine_tune\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        super().__init__(n_hiddens, k=k, loss_ae='MSELoss', loss_clf='CrossEntropyLoss',\n",
        "                         optimizer_ae='RMSprop', optimizer_clf='RMSprop',\n",
        "                         lr_rbm=lr_pre_train, lr_ae=lr_fine_tune, lr_clf=lr_fine_tune,\n",
        "                         epochs_rbm=epochs_pre_train, epochs_ae=epochs_fine_tune, epochs_clf=epochs_fine_tune,\n",
        "                         batch_size_rbm=batch_size, batch_size_ae=batch_size, batch_size_clf=batch_size,\n",
        "                         loss_ae_kwargs={}, loss_clf_kwargs={}, optimizer_ae_kwargs={},\n",
        "                         optimizer_clf_kwargs={}, random_state=random_state,\n",
        "                         use_gpu=use_gpu, verbose=verbose)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BtLDGKlxTP_e",
        "outputId": "23c95b61-0eff-4e27-eefa-fc79ff1c0720"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1797, 64)\n"
          ]
        }
      ],
      "source": [
        "X, y = load_digits(return_X_y=True)\n",
        "print(X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luFj5S3EU4CG"
      },
      "outputs": [],
      "source": [
        "def local_train(client_id, clf, X_local, y_local):\n",
        "    \"\"\"\n",
        "    Simulate local training on client with its own data.\n",
        "    \"\"\"\n",
        "    clf.fit(X_local, y_local)  # Fit the model locally on each client’s data\n",
        "    return clf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sdrgHRIFU-34"
      },
      "outputs": [],
      "source": [
        "# def federated_averaging(global_model, client_models):\n",
        "#     # Average the parameters of the encoder\n",
        "#     encoder_state_dict = OrderedDict()\n",
        "#     for key in client_models[0].cdbn_.encoder.state_dict().keys():\n",
        "#         encoder_state_dict[key] = sum(client_model.cdbn_.encoder.state_dict()[key] for client_model in client_models) / len(client_models)\n",
        "\n",
        "#     # Average the parameters of the output layer\n",
        "#     output_layer_state_dict = OrderedDict()\n",
        "#     for key in client_models[0].cdbn_.output_layer_.state_dict().keys():\n",
        "#         output_layer_state_dict[key] = sum(client_model.cdbn_.output_layer_.state_dict()[key] for client_model in client_models) / len(client_models)\n",
        "\n",
        "#     # Update the global model\n",
        "#     global_model.cdbn_ = copy.deepcopy(client_models[0].cdbn_)\n",
        "#     global_model.cdbn_.encoder.load_state_dict(encoder_state_dict)\n",
        "#     global_model.cdbn_.output_layer_.load_state_dict(output_layer_state_dict)\n",
        "\n",
        "#     # Average other attributes if necessary\n",
        "#     global_model.le_ = copy.deepcopy(client_models[0].le_)\n",
        "\n",
        "#     return global_model\n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "def federated_averaging(global_model, client_models, client_data_sizes):\n",
        "    # Calculate total data size across all clients\n",
        "    total_data_size = sum(client_data_sizes)\n",
        "\n",
        "    encoder_state_dict = OrderedDict()\n",
        "    output_layer_state_dict = OrderedDict()\n",
        "\n",
        "    # Weighted averaging of encoder parameters\n",
        "    for key in client_models[0].cdbn_.encoder.state_dict().keys():\n",
        "        encoder_state_dict[key] = sum(\n",
        "            (client_data_size / total_data_size) * client_model.cdbn_.encoder.state_dict()[key]\n",
        "            for client_model, client_data_size in zip(client_models, client_data_sizes)\n",
        "        )\n",
        "\n",
        "    # Weighted averaging of output layer parameters\n",
        "    for key in client_models[0].cdbn_.output_layer_.state_dict().keys():\n",
        "        output_layer_state_dict[key] = sum(\n",
        "            (client_data_size / total_data_size) * client_model.cdbn_.output_layer_.state_dict()[key]\n",
        "            for client_model, client_data_size in zip(client_models, client_data_sizes)\n",
        "        )\n",
        "\n",
        "    # Update the global model\n",
        "    global_model.cdbn_ = copy.deepcopy(client_models[0].cdbn_)\n",
        "    global_model.cdbn_.encoder.load_state_dict(encoder_state_dict)\n",
        "    global_model.cdbn_.output_layer_.load_state_dict(output_layer_state_dict)\n",
        "\n",
        "    return global_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbGJucbJVBwO"
      },
      "outputs": [],
      "source": [
        "def federated_training(clf, X, y, num_clients=3, rounds=10):\n",
        "    X_splits = np.array_split(X, num_clients)\n",
        "    y_splits = np.array_split(y, num_clients)\n",
        "    client_data_sizes = [len(y_local) for y_local in y_splits]\n",
        "\n",
        "    global_model = clone(clf)\n",
        "\n",
        "    # Initialize the global model\n",
        "    global_model.fit(X_splits[0], y_splits[0])\n",
        "\n",
        "    for round in range(rounds):\n",
        "        print(f\"Round {round+1}/{rounds}\")\n",
        "        local_models = []\n",
        "\n",
        "        for client_id in range(num_clients):\n",
        "            print(f\"Training on client {client_id+1}\")\n",
        "            local_model = clone(global_model)\n",
        "            local_model.fit(X_splits[client_id], y_splits[client_id])\n",
        "            local_models.append(local_model)\n",
        "\n",
        "        global_model = federated_averaging(global_model, local_models, client_data_sizes)\n",
        "\n",
        "    return global_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wST72VvdX0Ny"
      },
      "outputs": [],
      "source": [
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# param_grid = {\n",
        "#     'n_hiddens': [[500, 100, 20], [256, 128, 64], [1000, 500, 100]],\n",
        "#     'lr_rbm': [1e-5, 1e-4, 1e-3],  # Reduced range\n",
        "#     'lr_ae': [1e-4, 1e-3, 1e-2],   # Reduced range\n",
        "#     'lr_clf': [1e-4, 1e-3, 1e-2],  # Reduced range\n",
        "#     'epochs_rbm': [50, 100, 200],\n",
        "#     'epochs_ae': [25, 50, 100],\n",
        "#     'epochs_clf': [25, 50, 100],\n",
        "#     'batch_size_rbm': [30, 50, 100],\n",
        "#     'batch_size_ae': [30, 50, 100],\n",
        "#     'batch_size_clf': [30, 50, 100],\n",
        "#     'k': [1, 3, 5],\n",
        "#     'num_clients': [3, 5, 7],\n",
        "#     'rounds': [10, 15, 20]\n",
        "# }\n",
        "\n",
        "param_grid = {\n",
        "    'n_hiddens': [256,128,64],\n",
        "    'lr_rbm': 0.0001,\n",
        "    'lr_ae': 0.001,\n",
        "    'lr_clf': 0.01,\n",
        "    'epochs_rbm': 50,\n",
        "    'epochs_ae': 100,\n",
        "    'epochs_clf': 100,\n",
        "    'batch_size_rbm': 50,\n",
        "    'batch_size_ae': 30,\n",
        "    'batch_size_clf': 100,\n",
        "    'k': 3,\n",
        "    'num_clients': 3,\n",
        "    'rounds': 15\n",
        "}\n",
        "\n",
        "# param_combinations = list(ParameterGrid(param_grid))"
      ],
      "metadata": {
        "id": "IHi-3SxEAQTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def train_evaluate_model(X, y, params):\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    clf = DBNClassifier(\n",
        "        n_hiddens=params['n_hiddens'],\n",
        "        k=params['k'],\n",
        "        lr_rbm=params['lr_rbm'],\n",
        "        lr_ae=params['lr_ae'],\n",
        "        lr_clf=params['lr_clf'],\n",
        "        epochs_rbm=params['epochs_rbm'],\n",
        "        epochs_ae=params['epochs_ae'],\n",
        "        epochs_clf=params['epochs_clf'],\n",
        "        batch_size_rbm=params['batch_size_rbm'],\n",
        "        batch_size_ae=params['batch_size_ae'],\n",
        "        batch_size_clf=params['batch_size_clf'],\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    global_model = federated_training(\n",
        "        clf, X_train, y_train,\n",
        "        num_clients=params['num_clients'],\n",
        "        rounds=params['rounds']\n",
        "    )\n",
        "\n",
        "    y_pred = global_model.predict(X_val)\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "\n",
        "    return accuracy, global_model"
      ],
      "metadata": {
        "id": "1rMYRgrnAu-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# def hyperparameter_search(X, y, param_combinations, n_trials=10):\n",
        "#     best_accuracy = 0\n",
        "#     best_params = None\n",
        "#     best_model = None\n",
        "\n",
        "#     for i in range(n_trials):\n",
        "#         params = np.random.choice(param_combinations)\n",
        "#         accuracy, model = train_evaluate_model(X, y, params)\n",
        "\n",
        "#         print(f\"Trial {i+1}/{n_trials}: Accuracy = {accuracy}\")\n",
        "#         print(f\"Parameters: {params}\")\n",
        "\n",
        "#         if accuracy > best_accuracy:\n",
        "#             best_accuracy = accuracy\n",
        "#             best_params = params\n",
        "#             best_model = model\n",
        "\n",
        "#     return best_accuracy, best_params, best_model\n",
        "\n",
        "# # Perform the hyperparameter search\n",
        "# best_accuracy, best_params, best_model = hyperparameter_search(X, y, param_combinations, n_trials=10)\n",
        "\n",
        "# print(f\"\\nBest Accuracy: {best_accuracy}\")\n",
        "# print(f\"Best Parameters: {best_params}\")"
      ],
      "metadata": {
        "id": "Bz3D2JGtAzJM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform training and testing\n",
        "accuracy, model = train_evaluate_model(X, y, param_grid)\n",
        "\n",
        "# Display the results\n",
        "print(f\"Testing Accuracy: {accuracy}\")\n",
        "print(f\"Trained Model: {model}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUx6IzilWDFP",
        "outputId": "3020f4e0-70ef-4408-db63-af5ef314d664"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 2/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 3/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 4/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 5/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 6/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 7/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 8/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 9/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 10/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 11/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 12/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 13/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 14/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 15/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Testing Accuracy: 0.9166666666666666\n",
            "Trained Model: DBNClassifier(batch_size_clf=100, epochs_ae=100, epochs_clf=100, epochs_rbm=50,\n",
            "              lr_ae=0.001, lr_rbm=0.0001, n_hiddens=[256, 128, 64],\n",
            "              verbose=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 790
        },
        "id": "EtWn6aK4Tpgt",
        "outputId": "ce52a9e2-e971-405e-8f85-fca592cc0f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Round 1/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 2/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 3/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 4/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 5/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n",
            "Round 6/15\n",
            "Training on client 1\n",
            "Training on client 2\n",
            "Training on client 3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-9918c451839b>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m                     random_state=42, use_gpu=True, verbose=False)\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Federated learning simulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mglobal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfederated_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_clients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Global Model is ready!!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dda3608f5f97>\u001b[0m in \u001b[0;36mfederated_training\u001b[0;34m(clf, X, y, num_clients, rounds)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training on client {client_id+1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mlocal_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mlocal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_splits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_splits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mclient_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mlocal_models\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-31978f34083f>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m           )\n\u001b[1;32m     83\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maedbn_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maedbn_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfine_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m           \u001b[0;31m# use the trained encoder, add an output layer and perform a supervised fine-tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-c6cf9e42778a>\u001b[0m in \u001b[0;36mfine_tune\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m                 \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# clf = DBNClassifier(n_hiddens=[500, 100, 20], k=3,\n",
        "#                     loss_ae='MSELoss', loss_clf='CrossEntropyLoss',\n",
        "#                     optimizer_ae='RMSprop', optimizer_clf='RMSprop',\n",
        "#                     lr_rbm=1e-4, lr_ae=0.001, lr_clf=0.001,\n",
        "#                     epochs_rbm=100, epochs_ae=50, epochs_clf=50,\n",
        "#                     batch_size_rbm=50, batch_size_ae=30, batch_size_clf=50,\n",
        "#                     loss_ae_kwargs={}, loss_clf_kwargs={},\n",
        "#                     optimizer_ae_kwargs=dict(), optimizer_clf_kwargs=dict(),\n",
        "#                     random_state=42, use_gpu=True, verbose=False)\n",
        "# # Federated learning simulation\n",
        "# global_model = federated_training(clf, X_train, y_train, num_clients=3, rounds=15)\n",
        "# print(\"Global Model is ready!!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=model.predict(X_test)\n",
        "print(y_pred)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tnpe_VK1ytrw",
        "outputId": "a41341d4-cd22-47f1-de95-2faada442284"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6 9 3 7 2 2 5 2 5 2 1 4 4 0 4 2 3 7 8 8 4 3 9 7 5 6 3 5 6 3 4 9 1 4 4 6 9\n",
            " 4 7 6 6 9 1 3 6 1 3 0 6 5 5 1 9 5 6 0 9 0 0 1 0 4 5 2 4 5 7 0 7 5 9 9 5 4\n",
            " 7 0 4 5 9 9 9 0 2 3 8 0 6 4 4 9 1 2 8 3 9 2 9 4 4 4 4 3 5 3 1 8 5 1 4 2 7\n",
            " 7 4 4 1 9 2 7 8 7 2 6 9 4 2 7 2 7 5 8 7 5 7 7 0 6 6 4 2 8 0 9 4 6 9 9 6 9\n",
            " 0 1 5 6 6 0 6 4 2 9 3 7 7 2 9 0 4 5 3 6 5 7 9 8 4 2 1 3 7 7 2 2 3 9 8 0 3\n",
            " 2 2 5 6 9 9 4 1 5 4 2 3 6 4 8 5 9 5 7 1 9 4 8 1 5 4 4 9 6 1 2 6 0 4 5 2 7\n",
            " 4 6 4 5 6 0 3 2 3 6 7 1 9 2 4 7 6 8 2 5 5 1 6 2 8 8 8 5 7 6 2 2 2 3 4 8 8\n",
            " 3 6 0 3 7 7 0 1 0 4 5 1 5 3 6 0 4 1 0 0 3 6 5 9 7 3 5 9 9 9 8 5 3 3 2 0 5\n",
            " 8 3 4 0 2 4 6 4 3 4 5 0 5 2 1 3 1 4 1 1 7 0 8 5 2 1 2 8 7 0 6 4 8 1 5 1 8\n",
            " 4 5 8 7 9 8 6 0 4 2 0 7 9 2 9 5 2 7 7 9 9 7 4 3 7 3 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_score = model.score(X_test, y_test)\n",
        "print(f\"Test Score: {test_score}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJ5bDfaGy90l",
        "outputId": "0f298db9-03d8-495c-c572-d1f5bfc87fae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Score: 0.9166666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0f4Pb9Wm1wc"
      },
      "outputs": [],
      "source": [
        "# try:\n",
        "#     y_pred = global_model.predict(X_test)\n",
        "#     print(y_pred)\n",
        "# except Exception as e:\n",
        "#     print(f\"An error occurred during prediction: {e}\")\n",
        "#     import traceback\n",
        "#     traceback.print_exc()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "55xVgYYKVNDo"
      },
      "outputs": [],
      "source": [
        "# # Evaluate the global model on the test set\n",
        "# test_score = global_model.score(X_test, y_test)\n",
        "# print(f\"Test Score: {test_score}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}